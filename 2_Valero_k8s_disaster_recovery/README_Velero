# Velero for cluster disaster recovery with EKS cluster and with kops cluster.

NOTE: use the EKS controller course3_COMPLETE_kops_from_course8_project14_kops_snapshot in us-east-1 for all of the setups below.   







## EKS Cluster (using velero client and velero server, but using helm to install the velero server)


### backup and restore setup with Velero on EKS cluster



### cluster migration to new cluster









## kops cluster (using velero client and velero server)


### bring up the kops cluster in the course3_kops directory on the EC2 controller

export KOPS_CLUSTER_NAME=kops-project14.holinessinloveofchrist.com

export KOPS_STATE_STORE=s3://course3-kops-aws2-s3-state


kops create cluster --node-count=1 --node-size=t3.small --zones=us-east-1a --master-size=t3.small --master-zones=us-east-1a --ssh-public-key=course3_kops_from_course8_project14_EC2_instance_key.pub

kops update cluster --name kops-project14.holinessinloveofchrist.com --yes --admin

kubectl get node -o wide


### bring up the weatherapp using helm. This will be used to verify the backup and restore below via db corruption.


helm upgrade --install weatherapp-auth --set mysql.auth.rootPassword=mystrongpassword .


cd ..

helm upgrade --install weatherapp-ui .



cd ..


helm upgrade --install weatherapp-weather --set apikey=6eafbb0c45msh451df8302c291f2p1e0a35jsnf98732edea29 .


Verify the app is working by going to the LoadBalancer URL in kubectl get svc

Create a test user to be used later for the backup and restore test (user test)





### BACKUP AND RESTORE SETUP WITH VELERO ON kops CLUSTER:


#### Step 1:S3 bucket

First create the S3 bucket which will be used to store the backup state and the restore state of the cluster
aws s3 mb s3://recovery.holinessinloveofchrist.com --region us-east-1

##### Step 2: Velero Client installation

installing client side (CLI) 1.14.0 
wget https://github.com/vmware-tanzu/velero/releases/download/v1.14.0/velero-v1.14.0-linux-amd64.tar.gz
tar -xvf velero-v1.14.0-linux-amd64.tar.gz 
sudo mv velero-v1.14.0-linux-amd64/velero /usr/local/bin
velero version

velero version
Client:
        Version: v1.14.0
        Git commit: 2fc6300f2239f250b40b0488c35feae59520f2d3
<error getting server version: no matches for kind "ServerStatusRequest" in version "velero.io/v1">

The error is expected prior to installing the server



#### Step 3: Velero server installation

References:

https://velero.io/docs/v1.13/basic-install/#install-and-configure-the-server-components
https://velero.io/docs/v1.13/supported-providers/
https://github.com/vmware-tanzu/velero-plugin-for-aws#setup


server side:

1.10.0 is the latest plugin. This takes the place of the helm install with the values.yaml file in the video. This is ideal for a kops cluster as eksctl command cannot be used for the policy administration with kops.



Use the IAM method rather than kube2iam method in the links. This is the easiest way to do it.



create IAM user velero
aws iam create-user --user-name velero



attach the velero-policy.json policy to the velero user
note name is changed from velero_policy.json to velero-policy.json. It is the same file as used in the video, just replace the arn of your s3 bucket.


aws iam put-user-policy \
--user-name velero \
--policy-name velero \
--policy-document file://velero-policy.json


You will be able to see the IAM user and policy attached to it on AWS console to verify.


Create access keys for the velero user
aws iam create-access-key --user-name velero

Copy down the access key and secret access key


Create a file called credentials-velero

[default]
aws_access_key_id=<AWS_ACCESS_KEY_ID>
aws_secret_access_key=<AWS_SECRET_ACCESS_KEY>


Make sure the velero client has been downloaded and is working
velero version

velero version
Client:
        Version: v1.14.0
        Git commit: 2fc6300f2239f250b40b0488c35feae59520f2d3
<error getting server version: no matches for kind "ServerStatusRequest" in version "velero.io/v1">

The error is expected prior to installing the server



use velero install to install the server plugin using credentials-velero file created above

velero install \
--provider aws \
--plugins velero/velero-plugin-for-aws:v1.10.0 \
--bucket $BUCKET \
--backup-location-config region=$REGION \
--snapshot-location-config region=$REGION \
--secret-file ./credentials-velero


$BUCKET is just the bucket name, not the full arn
REGION is just the region, for example us-east-1



You should see a success message that server is now installed to the cluster

Deployment/velero: attempting to create resource client
Deployment/velero: created
Velero is installed! ⛵ Use 'kubectl logs deployment/velero -n velero' to view the status.



verify the install with kubectl get deployment -n velero
(the velero namespace is automatically created)


kubectl get deployment -n velero
NAME READY UP-TO-DATE AVAILABLE AGE
velero 1/1 1 1 40s

kubectl get ns
NAME STATUS AGE
default Active 3h18m
kube-node-lease Active 3h18m
kube-public Active 3h18m
kube-system Active 3h18m
velero Active 46s



verify the server pod is running with kubectl get pod -n velero

kubectl get pod -n velero
NAME READY STATUS RESTARTS AGE
velero-7fd78cc8dc-grjln 1/1 Running 0 72s





Velero version client should now show the server version


ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ velero version
Client:
        Version: v1.14.0
        Git commit: 2fc6300f2239f250b40b0488c35feae59520f2d3
Server:
        Version: v1.14.0



#### Step 4: Perform the backup


NOTE the app is running with a user test/test and login to the app is successful

ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ kubectl get pod
NAME                                  READY   STATUS    RESTARTS        AGE
weatherapp-auth-76c8df5d6b-k6f9h      1/1     Running   3 (9m26s ago)   10m
weatherapp-auth-76c8df5d6b-lxdkq      1/1     Running   3 (9m26s ago)   10m
weatherapp-auth-mysql-0               1/1     Running   0               10m
weatherapp-ui-5bd8dcc685-csqx5        1/1     Running   0               10m
weatherapp-ui-5bd8dcc685-jlr4g        1/1     Running   0               10m
weatherapp-weather-77b797b646-5fxdm   1/1     Running   0               9m35s
weatherapp-weather-77b797b646-s7k5c   1/1     Running   0               9m35s




velero backup create golden-backup2
Backup request "golden-backup2" submitted successfully.
Run `velero backup describe golden-backup2` or `velero backup logs golden-backup2` for more details.



velero backup describe golden-backup2
Name:         golden-backup2
Namespace:    velero
Labels:       velero.io/storage-location=default
Annotations:  velero.io/resource-timeout=10m0s
              velero.io/source-cluster-k8s-gitversion=v1.26.15
              velero.io/source-cluster-k8s-major-version=1
              velero.io/source-cluster-k8s-minor-version=26

Phase:  Completed


Namespaces:
  Included:  *
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  auto

Label selector:  <none>

Or label selector:  <none>

Storage Location:  default

Velero-Native Snapshot PVs:  auto
Snapshot Move Data:          false
Data Mover:                  velero

TTL:  720h0m0s

CSISnapshotTimeout:    10m0s
ItemOperationTimeout:  4h0m0s

Hooks:  <none>

Backup Format Version:  1.1.0

Started:    2024-07-13 00:56:04 +0000 UTC
Completed:  2024-07-13 00:56:08 +0000 UTC

Expiration:  2024-08-12 00:56:04 +0000 UTC

Total items to be backed up:  675
Items backed up:              675

Backup Volumes:
  Velero-Native Snapshots:
    pvc-8e0b07ce-33a1-46e2-9065-7847b44717b0: specify --details for more information

  CSI Snapshots: <none included>

  Pod Volume Backups: <none included>

HooksAttempted:  0
HooksFailed:     0



#### Step 5: Do an intentional database corruption to the mysql db in existing setup

kubectl exec -it weatherapp-auth-mysql-0 -- mysql -u root -p
Defaulted container "mysql" out of: mysql, preserve-logs-symlinks (init)
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 208
Server version: 8.4.0 Source distribution

Copyright (c) 2000, 2024, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| auth               |
| information_schema |
| my_database        |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
6 rows in set (0.02 sec)

mysql> drop database auth;
Query OK, 1 row affected (0.03 sec)

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| my_database        |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
5 rows in set (0.00 sec)



Confirm that user can no longer login.
"Invalid Credentials" shold be shown.


#### Step 6: Restore the setup with the velero backup

BEFORE restoring must do two things:

Delete the current stateful set and then delete the pvc::
This is because Velero does not restore items that are not missing. Remvoing the pvc will force velero to replace it from the backup when doing the retore (see further below)

kubectl get pvc
NAME                           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS    AGE
data-weatherapp-auth-mysql-0   Bound    pvc-8e0b07ce-33a1-46e2-9065-7847b44717b0   8Gi        RWO            kops-csi-1-21   21m

kubectl get sts
NAME                    READY   AGE
weatherapp-auth-mysql   1/1     22m


kubectl delete sts weatherapp-auth-mysql
statefulset.apps "weatherapp-auth-mysql" deleted

kubectl get sts
No resources found in default namespace.




 kubectl get pvc
NAME                           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS    AGE
data-weatherapp-auth-mysql-0   Bound    pvc-8e0b07ce-33a1-46e2-9065-7847b44717b0   8Gi        RWO            kops-csi-1-21   23m


kubectl delete pvc data-weatherapp-auth-mysql-0
persistentvolumeclaim "data-weatherapp-auth-mysql-0" deleted


kubectl get pvc
No resources found in default namespace.



NEXT:
Do the restoration::  (NOTE: there are some warnings as the complete setup is not able to be restored. Velero only restores missing/changed items not items that have not changed)



velero restore create golden-restore2 --from-backup golden-backup2
Restore request "golden-restore2" submitted successfully.
Run `velero restore describe golden-restore2` or `velero restore logs golden-restore2` for more details.



velero restore describe golden-restore2
Name:         golden-restore2
Namespace:    velero
Labels:       <none>
Annotations:  <none>

Phase:                       Completed
Total items to be restored:  408
Items restored:              408

Started:    2024-07-13 01:08:30 +0000 UTC
Completed:  2024-07-13 01:08:39 +0000 UTC

Warnings:
  Velero:     <none>
  Cluster:  could not restore, CustomResourceDefinition "backuprepositories.velero.io" already exists. Warning: the in-cluster version is different than the backed-up version
            
            <<< EDITED TO SAVE SPACE>>>
  Namespaces:
    kube-node-lease:  could not restore, Lease "i-074ff0302bda34b67" already exists. Warning: the in-cluster version is different than the backed-up version
                      could not restore, Lease "i-0e62e1d6955321f2d" already exists. Warning: the in-cluster version is different than the backed-up version
    kube-system:      could not restore, Pod "aws-cloud-controller-manager-wsttq" already exists. Warning: the in-cluster version is different than the backed-up version
                      
                      <<< EDITE TO SAVE SPACE>>>

Backup:  golden-backup2

Namespaces:
  Included:  all namespaces found in the backup
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io, csinodes.storage.k8s.io, volumeattachments.storage.k8s.io, backuprepositories.velero.io
  Cluster-scoped:  auto

Namespace mappings:  <none>

Label selector:  <none>

Or label selector:  <none>

Restore PVs:  auto

CSI Snapshot Restores: <none included>

Existing Resource Policy:   <none>
ItemOperationTimeout:       4h0m0s

Preserve Service NodePorts:  auto

Uploader config:


HooksAttempted:   0
HooksFailed:      0


#### Step 7: Test the app and the original user should be able to log in again.








### CLUSTER MIGRATION TO A NEW CLUSTER INFRA with kops clusters: Using Valero to clone a Kubernetes cluster (aka data migration)

NOTE: This is an Active-Backup failover migration strategy.  The first cluster should be deleted once the second cluster is up and running.   However, I noticed that there was a shared volumne between the two.  The volume was created by cluster1 but it was attacched to cluster2.  I had to delete cluster2 to free up the volune so that the kops delete cluster1 could complete. Otherwise, the volume remains undeleted.

NOTE: all of this is done on the EC2 controller that has the source code from git for the helm weatherapp, the code for velero migration (separate git repo), and the kops certificate in case SSH to the master or node is required.


#### Step 1: Assuming cluster1 is up and running, verify the cluster is registred as a context. The context command will be used to switch back and forth beween the clusters for administration purposes during the migration.

The two cluster commands are: 
kubectl config get-contexts and 
kubectl config use-context <cluster_name_from_get-contexts>

ubuntu@ip-172-31-21-52:~/course3_kops$ kubectl get node
NAME                  STATUS   ROLES           AGE    VERSION
i-02d85c0e26042fd74   Ready    node            2m     v1.26.15
i-03c391e4dfdf249e3   Ready    control-plane   4m5s   v1.26.15

ubuntu@ip-172-31-21-52:~/course3_kops$ kubectl config get-contexts
CURRENT   NAME                                        CLUSTER                                     AUTHINFO                                    NAMESPACE
*         kops-project14.holinessinloveofchrist.com   kops-project14.holinessinloveofchrist.com   kops-project14.holinessinloveofchrist.com   



#### Step 2: Install the helm weatherapp onto cluster1. This app will be used to test the migration of data from cluster1 to cluster2

Make sure in cluster1 context:

ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ kubectl config get-contexts
CURRENT   NAME                                        CLUSTER                                     AUTHINFO                                    NAMESPACE
*         kops-project14.holinessinloveofchrist.com   kops-project14.holinessinloveofchrist.com   kops-project14.holinessinloveofchrist.com   



ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab/source_files/weatherapp$ ls
ORIGINAL_SOURCE_helm_charts  auth     docker-compose.yaml    gitlab_kops_cert_user1  weatherapp-auth  weatherapp-weather
UI                           db-data  gitlab_kops_cert_user  weather                 weatherapp-ui

ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab/source_files/weatherapp$ pwd
/home/ubuntu/course3_projects/4_CICD_EKS_Gitlab/source_files/weatherapp





In weatherapp-auth:
helm upgrade --install weatherapp-auth --set mysql.auth.rootPassword=mystrongpassword .

verify the pods for the auth microservice come up:

ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab/source_files/weatherapp/weatherapp-auth$ kubectl get pod
NAME                               READY   STATUS     RESTARTS   AGE
weatherapp-auth-76c8df5d6b-bb2hf   0/1     Running    0          5s
weatherapp-auth-76c8df5d6b-wjktd   0/1     Running    0          5s
weatherapp-auth-mysql-0            0/1     Init:0/1   0          5s

ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab/source_files/weatherapp/weatherapp-auth$ kubectl get pod
NAME                               READY   STATUS    RESTARTS     AGE
weatherapp-auth-76c8df5d6b-bb2hf   1/1     Running   2 (7s ago)   67s
weatherapp-auth-76c8df5d6b-wjktd   1/1     Running   2 (7s ago)   67s
weatherapp-auth-mysql-0            1/1     Running   0            67s




In weatherapp-ui:
helm upgrade --install weatherapp-ui .

ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab/source_files/weatherapp/weatherapp-ui$ kubectl get pod
NAME                               READY   STATUS              RESTARTS      AGE
weatherapp-auth-76c8df5d6b-bb2hf   1/1     Running             2 (70s ago)   2m10s
weatherapp-auth-76c8df5d6b-wjktd   1/1     Running             2 (70s ago)   2m10s
weatherapp-auth-mysql-0            1/1     Running             0             2m10s
weatherapp-ui-5bd8dcc685-gb6sc     0/1     ContainerCreating   0             5s
weatherapp-ui-5bd8dcc685-smwbd     0/1     ContainerCreating   0             5s

ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab/source_files/weatherapp/weatherapp-ui$ kubectl get pod
NAME                               READY   STATUS    RESTARTS      AGE
weatherapp-auth-76c8df5d6b-bb2hf   1/1     Running   2 (89s ago)   2m29s
weatherapp-auth-76c8df5d6b-wjktd   1/1     Running   2 (89s ago)   2m29s
weatherapp-auth-mysql-0            1/1     Running   0             2m29s
weatherapp-ui-5bd8dcc685-gb6sc     1/1     Running   0             24s
weatherapp-ui-5bd8dcc685-smwbd     1/1     Running   0             24s



In weatherapp-weather:
helm upgrade --install weatherapp-weather --set apikey=6eafbb0c45msh451df8302c291f2p1e0a35jsnf98732edea29 .

ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab/source_files/weatherapp/weatherapp-weather$ kubectl get pod
NAME                                  READY   STATUS              RESTARTS       AGE
weatherapp-auth-76c8df5d6b-bb2hf      1/1     Running             2 (2m6s ago)   3m6s
weatherapp-auth-76c8df5d6b-wjktd      1/1     Running             2 (2m6s ago)   3m6s
weatherapp-auth-mysql-0               1/1     Running             0              3m6s
weatherapp-ui-5bd8dcc685-gb6sc        1/1     Running             0              61s
weatherapp-ui-5bd8dcc685-smwbd        1/1     Running             0              61s
weatherapp-weather-77b797b646-2lkvf   0/1     ContainerCreating   0              5s
weatherapp-weather-77b797b646-sd4w7   0/1     ContainerCreating   0              5s

ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab/source_files/weatherapp/weatherapp-weather$ kubectl get pod
NAME                                  READY   STATUS    RESTARTS        AGE
weatherapp-auth-76c8df5d6b-bb2hf      1/1     Running   2 (2m20s ago)   3m20s
weatherapp-auth-76c8df5d6b-wjktd      1/1     Running   2 (2m20s ago)   3m20s
weatherapp-auth-mysql-0               1/1     Running   0               3m20s
weatherapp-ui-5bd8dcc685-gb6sc        1/1     Running   0               75s
weatherapp-ui-5bd8dcc685-smwbd        1/1     Running   0               75s
weatherapp-weather-77b797b646-2lkvf   1/1     Running   0               19s
weatherapp-weather-77b797b646-sd4w7   1/1     Running   0               19s



#### Step 3: Check login to the weatherapp in cluster1 and create a user 

ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab/source_files/weatherapp/weatherapp-weather$ kubectl get svc
NAME                             TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT(S)        AGE
kubernetes                       ClusterIP      100.64.0.1       <none>                                                                   443/TCP        18m
weatherapp-auth                  ClusterIP      100.64.94.78     <none>                                                                   8080/TCP       4m19s
weatherapp-auth-mysql            ClusterIP      100.70.191.58    <none>                                                                   3306/TCP       4m19s
weatherapp-auth-mysql-headless   ClusterIP      None             <none>                                                                   3306/TCP       4m19s
weatherapp-ui                    LoadBalancer   100.68.239.103   a0fc5ef61e71e46f69a72950d432e795-504550650.us-east-1.elb.amazonaws.com   80:32292/TCP   2m14s
weatherapp-weather               ClusterIP      100.71.159.121   <none>                                                                   5000/TCP       78s


#### Step 4: The velero client is already installed on the controller. If it is not then install it. See previous section on BACKUP and RESTORE with the kops cluster above.

installing client side (CLI) 1.14.0  (per video)
wget https://github.com/vmware-tanzu/velero/releases/download/v1.14.0/velero-v1.14.0-linux-amd64.tar.gz
tar -xvf velero-v1.14.0-linux-amd64.tar.gz 
sudo mv velero-v1.14.0-linux-amd64/velero /usr/local/bin
velero version


velero version will show error for server until the server side is installed. see next step below.

velero version
Client:
        Version: v1.14.0
        Git commit: 2fc6300f2239f250b40b0488c35feae59520f2d3
<error getting server version: no matches for kind "ServerStatusRequest" in version "velero.io/v1">




#### Step 5: Install the velero server on the cluster. This has to be done on each new cluster setup. Use the velero-policy.json file that does not have the ec2:AttachVolume permission.  

ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ ls
credentials-velero  velero-policy-addAttachVolume.json  velero-policy.json


ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ kubectl config get-contexts
CURRENT   NAME                                        CLUSTER                                     AUTHINFO                                    NAMESPACE
*         kops-project14.holinessinloveofchrist.com   kops-project14.holinessinloveofchrist.com   kops-project14.holinessinloveofchrist.com   

1.10.0 is the latest plugin.  This takes the place of the helm install with the values.yaml file in the video.  This is ideal for a kops cluster as eksctl command cannot be used for the policy administration with kops.

Instructional links: (See also previous section on BACKUP and RESTORE with kops cluster)
https://velero.io/docs/v1.13/basic-install/#install-and-configure-the-server-components
https://velero.io/docs/v1.13/supported-providers/
https://github.com/vmware-tanzu/velero-plugin-for-aws#setup

The IAM credentials of the velero IAM user is in the file credentials-velero

The velero-policy.json file has been attached to the velero user (see previous section on BACKUP and RESTORE)

There is no ec2:AttachVolume on this policy. That will be added to the kops configuration for cluster2 (see later step)

use velero client "velero install" to install the server plugin using credentials-velero file created above

NOTE: Install Velero, including all prerequisites, into the cluster and start the deployment. This will create a namespace called velero, and place a deployment named velero in it.
The namespace will automatically be created. The deployment will be named velero.

velero install \
    --provider aws \
    --plugins velero/velero-plugin-for-aws:v1.10.0 \
    --bucket recovery.holinessinloveofchrist.com \
    --backup-location-config region=us-east-1 \
    --snapshot-location-config region=us-east-1 \
    --secret-file ./credentials-velero


ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ velero version
Client:
        Version: v1.14.0
        Git commit: 2fc6300f2239f250b40b0488c35feae59520f2d3
Server:
        Version: v1.14.0



#### Step 6:  Create the backup of cluster1. This will back up the PVC bound volume so that it can be copied to a new volume that cluster2 will use.


ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ velero backup create golden-backu4
Backup request "golden-backu4" submitted successfully.
Run `velero backup describe golden-backu4` or `velero backup logs golden-backu4` for more details.



ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ velero backup describe golden-backu4
Name:         golden-backu4
Namespace:    velero
Labels:       velero.io/storage-location=default
Annotations:  velero.io/resource-timeout=10m0s
              velero.io/source-cluster-k8s-gitversion=v1.26.15
              velero.io/source-cluster-k8s-major-version=1
              velero.io/source-cluster-k8s-minor-version=26

Phase:  Completed


Namespaces:
  Included:  *
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  auto

Label selector:  <none>

Or label selector:  <none>

Storage Location:  default

Velero-Native Snapshot PVs:  auto
Snapshot Move Data:          false
Data Mover:                  velero

TTL:  720h0m0s

CSISnapshotTimeout:    10m0s
ItemOperationTimeout:  4h0m0s

Hooks:  <none>

Backup Format Version:  1.1.0

Started:    2024-07-15 19:33:15 +0000 UTC
Completed:  2024-07-15 19:33:19 +0000 UTC

Expiration:  2024-08-14 19:33:15 +0000 UTC

Total items to be backed up:  675
Items backed up:              675

Backup Volumes:
  Velero-Native Snapshots:
    pvc-4829409a-b8d0-4ee0-aeea-ac56a5dbe88c: specify --details for more information

  CSI Snapshots: <none included>

  Pod Volume Backups: <none included>

HooksAttempted:  0
HooksFailed:     0


This backup shows up in the AWS console in the recovery.holinessinloveofchrist.com S3 bucket under the backups folder.


#### Step 7: Create a new kops cluster2 that will be used to migrate the cluster1 to.

In a NEW terminal set the ENV vars for the new cluster. 

NOTE: A new kops cluster bucket is created for this cluster for the kops cluster state. This is not related to the recovery S3 bucket that is used for the data migration using velero.

The Route53 hosted zone is created for this new cluster. Test the dig NS for reachablility.

dig NS cluster2.holinessinloveofchrist.com

; <<>> DiG 9.10.6 <<>> NS cluster2.holinessinloveofchrist.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 13957
;; flags: qr rd ra; QUERY: 1, ANSWER: 4, AUTHORITY: 0, ADDITIONAL: 9

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 512
;; QUESTION SECTION:
;cluster2.holinessinloveofchrist.com. IN        NS

;; ANSWER SECTION:
cluster2.holinessinloveofchrist.com. 172800 IN NS ns-470.awsdns-58.com.
cluster2.holinessinloveofchrist.com. 172800 IN NS ns-574.awsdns-07.net.
cluster2.holinessinloveofchrist.com. 172800 IN NS ns-1526.awsdns-62.org.
cluster2.holinessinloveofchrist.com. 172800 IN NS ns-2017.awsdns-60.co.uk.

;; ADDITIONAL SECTION:
ns-1526.awsdns-62.org.  47147   IN      A       205.251.197.246
ns-1526.awsdns-62.org.  48952   IN      AAAA    2600:9000:5305:f600::1
ns-2017.awsdns-60.co.uk. 48115  IN      A       205.251.199.225
ns-2017.awsdns-60.co.uk. 50730  IN      AAAA    2600:9000:5307:e100::1
ns-470.awsdns-58.com.   47256   IN      A       205.251.193.214
ns-470.awsdns-58.com.   49121   IN      AAAA    2600:9000:5301:d600::1
ns-574.awsdns-07.net.   47948   IN      A       205.251.194.62
ns-574.awsdns-07.net.   49862   IN      AAAA    2600:9000:5302:3e00::1

;; Query time: 495 msec
;; SERVER: 2001:558:feed::1#53(2001:558:feed::1)
;; WHEN: Mon Jul 15 12:39:57 PDT 2024
;; MSG SIZE  rcvd: 377

export KOPS_CLUSTER_NAME=cluster2.holinessinloveofchrist.com

export KOPS_STATE_STORE=s3://course3-kops-cluster2-aws2-s3-state
enable bucket versioning

kops create cluster --node-count=1 --node-size=t3.small --zones=us-east-1a --master-size=t3.small --master-zones=us-east-1a --ssh-public-key=course3_kops_from_course8_project14_EC2_instance_key.pub

kops update cluster --name cluster2.holinessinloveofchrist.com --yes --admin

kops validate cluster

kops delete cluster --yes



NOTE: the kops commands are run outside of the kubectl context setting.   As long as the ENV vars are set properly in the terminal the kops commands apply to the cluster specified in the ENV vars and are not applied to the current context setting.

#### Step 8: Verfiy that there are now 2 kubeconfig kubectl contexts concurrently running


ubuntu@ip-172-31-21-52:~/course3_kops$ kubectl config get-contexts
CURRENT   NAME                                        CLUSTER                                     AUTHINFO                                    NAMESPACE
*         cluster2.holinessinloveofchrist.com         cluster2.holinessinloveofchrist.com         cluster2.holinessinloveofchrist.com         
          kops-project14.holinessinloveofchrist.com   kops-project14.holinessinloveofchrist.com   kops-project14.holinessinloveofchrist.com   


The current context is set to cluster2

ubuntu@ip-172-31-21-52:~/course3_kops$ kubectl get node
NAME                  STATUS   ROLES           AGE     VERSION
i-0b032cc67827546f0   Ready    node            5m17s   v1.26.15
i-0bd1bd17b637c7349   Ready    control-plane   7m48s   v1.26.15

NOTE that the nodes are distinct from the cluster1 nodes above: these are the cluster1  nodes

ubuntu@ip-172-31-21-52:~/course3_kops$ kubectl get node
NAME                  STATUS   ROLES           AGE    VERSION
i-02d85c0e26042fd74   Ready    node            2m     v1.26.15
i-03c391e4dfdf249e3   Ready    control-plane   4m5s   v1.26.15






#### Step 9: For cluster2: Update the kops configuration adding the ec2:AttachVolume and other permissions in the addtionalPolicies section of the kops configuration. Make sure to apply the update

NOTE: The ec2:AttachVolume is required with cluster2 because unlike cluster1, the weatherapp is not installed onto cluster2 with helm.  And there is no state in cluster2 for the attachVolume of the new volume during the migration with Velero. It must be manually attached by the kops master after Velero instructs the master what needs to be done and what backup is to be used to "restore" (migrate) to the cluster2.

This is the kops additionalPolicies block that needs to be added to the "kops edit cluster" configuration file:

Make sure this is done in the cluster2 terminal with the KOPS cluster ENV var set to cluster2.

It only needs to be applied to clsuter2 kops configuration
"kops edit cluster" is the command to get into the editor.






metadata:
  creationTimestamp: "2024-07-05T21:05:46Z"
  name: kops-project14.holinessinloveofchrist.com
spec:
  additionalPolicies:    <<<<<<<<<< instert it here in spec
    node: |
      [
        {
          "Effect": "Allow",
          "Action": [
            "ec2:AttachVolume",
            "ec2:CreateSnapshot",
            "ec2:CreateTags",
            "ec2:CreateVolume",
            "ec2:DeleteSnapshot",
            "ec2:DeleteTags",
            "ec2:DeleteVolume",
            "ec2:DescribeAvailabilityZones",
            "ec2:DescribeInstances",
            "ec2:DescribeSnapshots",
            "ec2:DescribeTags",
            "ec2:DescribeVolumes",
            "ec2:DescribeVolumesModifications",
            "ec2:DetachVolume",
            "ec2:ModifyVolume"
          ],
          "Resource": "*"
        }
      ]
    master: |
      [
        {
          "Effect": "Allow",
          "Action": [
            "ec2:AttachVolume",
            "ec2:CreateSnapshot",
            "ec2:CreateTags",
            "ec2:CreateVolume",
            "ec2:DeleteSnapshot",
            "ec2:DeleteTags",
            "ec2:DeleteVolume",
            "ec2:DescribeAvailabilityZones",
            "ec2:DescribeInstances",
            "ec2:DescribeSnapshots",
            "ec2:DescribeTags",
            "ec2:DescribeVolumes",
            "ec2:DescribeVolumesModifications",
            "ec2:DetachVolume",
            "ec2:ModifyVolume"
          ],
          "Resource": "*"
        }
      ]
  api:     <<<<< insert it before api block
    dns: {}



#### Step 10: Make sure to do a kops update cluster to update the kops configuration with the above policy. This only needs to be done on cluster2


ubuntu@ip-172-31-21-52:~/course3_kops$ kops edit cluster
ubuntu@ip-172-31-21-52:~/course3_kops$ kops update cluster

*********************************************************************************

A new kops version is available: 1.28.4
Upgrading is recommended
More information: https://github.com/kubernetes/kops/blob/master/permalinks/upgrade_kops.md#1.28.4

*********************************************************************************

I0715 19:58:11.559657    1511 executor.go:111] Tasks: 0 done / 93 total; 46 can run
I0715 19:58:12.119913    1511 executor.go:111] Tasks: 46 done / 93 total; 19 can run
I0715 19:58:12.383772    1511 executor.go:111] Tasks: 65 done / 93 total; 24 can run
I0715 19:58:12.863421    1511 executor.go:111] Tasks: 89 done / 93 total; 2 can run
I0715 19:58:12.980113    1511 executor.go:111] Tasks: 91 done / 93 total; 2 can run
I0715 19:58:13.093654    1511 executor.go:111] Tasks: 93 done / 93 total; 0 can run
Will create resources:
  IAMRolePolicy/additional.masters.cluster2.holinessinloveofchrist.com
        Role                    name:masters.cluster2.holinessinloveofchrist.com id:AROAYS2NRU3C2QUNI6ZIN
        Managed                 false

  IAMRolePolicy/additional.nodes.cluster2.holinessinloveofchrist.com
        Role                    name:nodes.cluster2.holinessinloveofchrist.com id:AROAYS2NRU3CYADFXKHMD
        Managed                 false

Will modify resources:
  ManagedFile/cluster-completed.spec
        Contents            
                                ...
                                  metadata:
                                    creationTimestamp: "2024-07-15T19:43:19Z"
                                +   generation: 1
                                    name: cluster2.holinessinloveofchrist.com
                                  spec:
                                +   additionalPolicies:
                                +     master: |
                                +       [
                                +         {
                                +           "Effect": "Allow",
                                +           "Action": [
                                +             "ec2:AttachVolume",
                                +             "ec2:CreateSnapshot",
                                +             "ec2:CreateTags",
                                +             "ec2:CreateVolume",
                                +             "ec2:DeleteSnapshot",
                                +             "ec2:DeleteTags",
                                +             "ec2:DeleteVolume",
                                +             "ec2:DescribeAvailabilityZones",
                                +             "ec2:DescribeInstances",
                                +             "ec2:DescribeSnapshots",
                                +             "ec2:DescribeTags",
                                +             "ec2:DescribeVolumes",
                                +             "ec2:DescribeVolumesModifications",
                                +             "ec2:DetachVolume",
                                +             "ec2:ModifyVolume"
                                +           ],
                                +           "Resource": "*"
                                +         }
                                +       ]
                                +     node: |
                                +       [
                                +         {
                                +           "Effect": "Allow",
                                +           "Action": [
                                +             "ec2:AttachVolume",
                                +             "ec2:CreateSnapshot",
                                +             "ec2:CreateTags",
                                +             "ec2:CreateVolume",
                                +             "ec2:DeleteSnapshot",
                                +             "ec2:DeleteTags",
                                +             "ec2:DeleteVolume",
                                +             "ec2:DescribeAvailabilityZones",
                                +             "ec2:DescribeInstances",
                                +             "ec2:DescribeSnapshots",
                                +             "ec2:DescribeTags",
                                +             "ec2:DescribeVolumes",
                                +             "ec2:DescribeVolumesModifications",
                                +             "ec2:DetachVolume",
                                +             "ec2:ModifyVolume"
                                +           ],
                                +           "Resource": "*"
                                +         }
                                +       ]
                                    api:
                                      dns: {}
                                ...
                            

Must specify --yes to apply changes
ubuntu@ip-172-31-21-52:~/course3_kops$ kops update cluster --yes

*********************************************************************************

A new kops version is available: 1.28.4
Upgrading is recommended
More information: https://github.com/kubernetes/kops/blob/master/permalinks/upgrade_kops.md#1.28.4

*********************************************************************************

I0715 19:58:22.700760    1516 executor.go:111] Tasks: 0 done / 93 total; 46 can run
I0715 19:58:23.024370    1516 executor.go:111] Tasks: 46 done / 93 total; 19 can run
I0715 19:58:23.224116    1516 executor.go:111] Tasks: 65 done / 93 total; 24 can run
I0715 19:58:23.678474    1516 executor.go:111] Tasks: 89 done / 93 total; 2 can run
I0715 19:58:23.737970    1516 executor.go:111] Tasks: 91 done / 93 total; 2 can run
I0715 19:58:23.780906    1516 executor.go:111] Tasks: 93 done / 93 total; 0 can run
I0715 19:58:23.851741    1516 dns.go:232] Pre-creating DNS records
I0715 19:58:23.968895    1516 update_cluster.go:323] Exporting kubeconfig for cluster
kOps has set your kubectl context to cluster2.holinessinloveofchrist.com
W0715 19:58:24.014145    1516 update_cluster.go:347] Exported kubeconfig with no user authentication; use --admin, --user or --auth-plugin flags with `kops export kubeconfig`

Cluster changes have been applied to the cloud.


Changes may require instances to restart: kops rolling-update cluster





#### Step 11: Install the velero server onto this new cluster. 

NOTE: This is a cluster context configuration senstive operation. You must switch contexts to the cluster2 prior to installing the velero server plugin. Otherwise it will be applied to cluster1 (which already has the velero server installed on it)


ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ kubectl config get-contexts
CURRENT   NAME                                        CLUSTER                                     AUTHINFO                                    NAMESPACE
*         cluster2.holinessinloveofchrist.com         cluster2.holinessinloveofchrist.com         cluster2.holinessinloveofchrist.com         
          kops-project14.holinessinloveofchrist.com   kops-project14.holinessinloveofchrist.com   kops-project14.holinessinloveofchrist.com   

ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ ls
credentials-velero  velero-policy-addAttachVolume.json  velero-policy.json




ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ velero version
Client:
        Version: v1.14.0
        Git commit: 2fc6300f2239f250b40b0488c35feae59520f2d3
Server:
        Version: v1.14.0



ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ velero install \
    --provider aws \
    --plugins velero/velero-plugin-for-aws:v1.10.0 \
    --bucket recovery.holinessinloveofchrist.com \
    --backup-location-config region=us-east-1 \
    --snapshot-location-config region=us-east-1 \
    --secret-file ./credentials-velero



#### Step 12: At this point the backup from cluster1 can be applied to the cluster2 setup.

velero restore create golden-restore8-cluster2 --from-backup golden-backu4


ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ velero restore describe golden-restore8-cluster2
Name:         golden-restore8-cluster2
Namespace:    velero
Labels:       <none>
Annotations:  <none>

Phase:                       Finalizing
Total items to be restored:  407
Items restored:              407

Started:    2024-07-15 20:05:45 +0000 UTC
Completed:  <n/a>



#### Step 13: Verify that the pods are up and running on the new cluster after the data migration above from cluster1 to cluster2


cluster2:

ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ kubectl get pods
NAME                                  READY   STATUS    RESTARTS      AGE
weatherapp-auth-76c8df5d6b-bb2hf      0/1     Running   3 (14s ago)   64s <<<<< 64 seconds
weatherapp-auth-76c8df5d6b-wjktd      0/1     Running   3 (13s ago)   64s
weatherapp-auth-mysql-0               0/1     Running   0             64s
weatherapp-ui-5bd8dcc685-gb6sc        1/1     Running   0             64s
weatherapp-ui-5bd8dcc685-smwbd        1/1     Running   0             63s
weatherapp-weather-77b797b646-2lkvf   1/1     Running   0             63s
weatherapp-weather-77b797b646-sd4w7   1/1     Running   0             63s


NOTE that the pods are named the same as those running in cluster1, but are distinct instances each in their own context (see timestamp of the pods)


cluster1: 

ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ kubectl config use-context kops-project14.holinessinloveofchrist.com 
Switched to context "kops-project14.holinessinloveofchrist.com".
ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ kubectl get pod
NAME                                  READY   STATUS    RESTARTS      AGE
weatherapp-auth-76c8df5d6b-bb2hf      1/1     Running   2 (57m ago)   58m  <<<<<< 58 minutes
weatherapp-auth-76c8df5d6b-wjktd      1/1     Running   2 (57m ago)   58m
weatherapp-auth-mysql-0               1/1     Running   0             58m
weatherapp-ui-5bd8dcc685-gb6sc        1/1     Running   0             56m
weatherapp-ui-5bd8dcc685-smwbd        1/1     Running   0             56m
weatherapp-weather-77b797b646-2lkvf   1/1     Running   0             55m
weatherapp-weather-77b797b646-sd4w7   1/1     Running   0             55m





The PVC is reinitiated and is the same PVC for both

ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ kubectl config use-context kops-project14.holinessinloveofchrist.com 
Switched to context "kops-project14.holinessinloveofchrist.com".
ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ kubectl get pod
NAME                                  READY   STATUS    RESTARTS      AGE
weatherapp-auth-76c8df5d6b-bb2hf      1/1     Running   2 (57m ago)   58m
weatherapp-auth-76c8df5d6b-wjktd      1/1     Running   2 (57m ago)   58m
weatherapp-auth-mysql-0               1/1     Running   0             58m
weatherapp-ui-5bd8dcc685-gb6sc        1/1     Running   0             56m
weatherapp-ui-5bd8dcc685-smwbd        1/1     Running   0             56m
weatherapp-weather-77b797b646-2lkvf   1/1     Running   0             55m
weatherapp-weather-77b797b646-sd4w7   1/1     Running   0             55m
ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ kubectl get pvc
NAME                           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS    AGE
data-weatherapp-auth-mysql-0   Bound    pvc-4829409a-b8d0-4ee0-aeea-ac56a5dbe88c   8Gi        RWO            kops-csi-1-21   61m
ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ kubectl config use-context cluster2.holinessinloveofchrist.com
Switched to context "cluster2.holinessinloveofchrist.com".
ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ kubectl get pvc
NAME                           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS    AGE
data-weatherapp-auth-mysql-0   Bound    pvc-4829409a-b8d0-4ee0-aeea-ac56a5dbe88c   8Gi        RWO            kops-csi-1-21   6m1s



#### Step 14: Verify the distinct volumes that are used by the clusters. Even though the PVC is the same, the volume binding to the respective nodes are unique.  In EBS, the Access Mode is only RWO so distinct volumes are used for each cluster and the nodes in the cluster. The csi controller pods are unique for each cluster.

cluster2:

In the kubectl logs ebs-csi-controller-77c5d76dd4-8lhc2 -n kube-system

I0715 20:06:27.144518       1 controller.go:353] [Debug] ControllerPublishVolume: volume vol-09984553ed3a1ee73 attached to node i-0b032cc67827546f0 through device /dev/xvdba



cluster1:

In the kubectl logs ebs-csi-controller-55cb6776b4-r8hw2  -n kube-system

I0715 19:09:26.486912       1 controller.go:353] [Debug] ControllerPublishVolume: volume vol-020177b47e873732e attached to node i-02d85c0e26042fd74 through device /dev/xvdba


In AWS console:

The volumes are unique (see above) but the name of the volumes is cluster1 for both.   This is because the volume contents  of cluster1 is copied by to the new volume used by cluster2, since velero backup is from clsuter1, and the name gets transferred with it.

This creates a race condition when deleting cluster1.  Cluster1 kops thinks it “owns” the cluster2 volume because of the name cluster1, and it tries to delete it, but I cannot because it is being used by cluster2.

I had to delete cluster2 in order for cluster1 kops to complete the delete. In reality the cluster1 delete ends up exiting without deleting the volume after about 10 minutes of trying and I guess that’s ok because it appears that all of the other cluster1 stuff is successfully deleted.


#### Step 15: Verify that cluster2 weatherapp can be reached via browser and that the test user login works from the restore of cluster1 database.



Cluster2: 
ubuntu@ip-172-31-21-52:~/course3_kops$ kubectl config use-context  cluster2.holinessinloveofchrist.com 
Switched to context "cluster2.holinessinloveofchrist.com".
ubuntu@ip-172-31-21-52:~/course3_kops$ kubectl get pods
NAME                                  READY   STATUS    RESTARTS      AGE
weatherapp-auth-76c8df5d6b-bb2hf      1/1     Running   5 (20m ago)   28m
weatherapp-auth-76c8df5d6b-wjktd      1/1     Running   5 (20m ago)   28m
weatherapp-auth-mysql-0               1/1     Running   0             28m
weatherapp-ui-5bd8dcc685-gb6sc        1/1     Running   1 (20m ago)   28m
weatherapp-ui-5bd8dcc685-smwbd        1/1     Running   1 (20m ago)   28m
weatherapp-weather-77b797b646-2lkvf   1/1     Running   1 (20m ago)   28m
weatherapp-weather-77b797b646-sd4w7   1/1     Running   1 (20m ago)   28m

ubuntu@ip-172-31-21-52:~/course3_kops$ kubectl get svc
NAME                             TYPE           CLUSTER-IP       EXTERNAL-IP                                                               PORT(S)        AGE
kubernetes                       ClusterIP      100.64.0.1       <none>                                                                    443/TCP        47m
weatherapp-auth                  ClusterIP      100.66.169.126   <none>                                                                    8080/TCP       28m
weatherapp-auth-mysql            ClusterIP      100.69.23.150    <none>                                                                    3306/TCP       28m
weatherapp-auth-mysql-headless   ClusterIP      None             <none>                                                                    3306/TCP       28m
weatherapp-ui                    LoadBalancer   100.68.198.20    a059aceca432d43c084cf7db4efbd43c-1660135991.us-east-1.elb.amazonaws.com   80:30952/TCP   28m
weatherapp-weather               ClusterIP      100.67.179.100   <none>                                                                    5000/TCP       28m



NOTE: the svc LoadBalancer URL is unique for each cluster as it should be.

For Cluster1 see below:
ubuntu@ip-172-31-21-52:~/course3_kops$ kubectl config use-context kops-project14.holinessinloveofchrist.com
Switched to context "kops-project14.holinessinloveofchrist.com".
ubuntu@ip-172-31-21-52:~/course3_kops$ kubectl get svc
NAME                             TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT(S)        AGE
kubernetes                       ClusterIP      100.64.0.1       <none>                                                                   443/TCP        99m
weatherapp-auth                  ClusterIP      100.64.94.78     <none>                                                                   8080/TCP       85m
weatherapp-auth-mysql            ClusterIP      100.70.191.58    <none>                                                                   3306/TCP       85m
weatherapp-auth-mysql-headless   ClusterIP      None             <none>                                                                   3306/TCP       85m
weatherapp-ui                    LoadBalancer   100.68.239.103   a0fc5ef61e71e46f69a72950d432e795-504550650.us-east-1.elb.amazonaws.com   80:32292/TCP   83m
weatherapp-weather               ClusterIP      100.71.159.121   <none>                                                                   5000/TCP       82m




FOR CLUSTER2: this is the URL
a059aceca432d43c084cf7db4efbd43c-1660135991.us-east-1.elb.amazonaws.com

The login to the test user is successful indicating that the data migration is successful.






#### Step 16: As noted above there is a race condtion with the deletion of cluster1 after cluster2 is configured/migration is complete

cluster2:

In the kubectl logs ebs-csi-controller-77c5d76dd4-8lhc2 -n kube-system

I0715 20:06:27.144518       1 controller.go:353] [Debug] ControllerPublishVolume: volume vol-09984553ed3a1ee73 attached to node i-0b032cc67827546f0 through device /dev/xvdba



cluster1:

In the kubectl logs ebs-csi-controller-55cb6776b4-r8hw2  -n kube-system

I0715 19:09:26.486912       1 controller.go:353] [Debug] ControllerPublishVolume: volume vol-020177b47e873732e attached to node i-02d85c0e26042fd74 through device /dev/xvdba


In AWS console:

The volumes are unique (see above) but the name of the volumes is cluster1 for both.   This is because the volume contents  of cluster1 is copied by to the new volume used by cluster2, since velero backup is from clsuter1, and the name gets transferred with it.

This creates a race condition when deleting cluster1.  Cluster1 kops thinks it “owns” the cluster2 volume because of the name cluster1, and it tries to delete it, but I cannot because it is being used by cluster2.

I had to delete cluster2 in order for cluster1 kops to complete the delete. In reality the cluster1 delete ends up exiting without deleting the volume after about 10 minutes of trying and I guess that’s ok because it appears that all of the other cluster1 stuff is successfully deleted.




ubuntu@ip-172-31-21-52:~/course3_projects/5_Disaster_Recovery/2_Valero_k8s_disaster_recovery/kops_method$ ls -la
total 20
drwxrwxr-x 2 ubuntu ubuntu 4096 Jul 15 02:34 .
drwxrwxr-x 4 ubuntu ubuntu 4096 Jul 13 03:09 ..
-rw-rw-r-- 1 ubuntu ubuntu  112 Jul 12 02:19 credentials-velero
-rw-rw-r-- 1 ubuntu ubuntu 1019 Jul 15 02:34 velero-policy-addAttachVolume.json
-rw-rw-r-- 1 ubuntu ubuntu 1019 Jul 12 02:18 velero-policy.json





